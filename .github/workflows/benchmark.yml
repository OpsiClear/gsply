name: Benchmark

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  workflow_dispatch:  # Allow manual triggering

jobs:
  benchmark:
    name: Run benchmarks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[benchmark,jit]"

    - name: Verify numba installation
      run: |
        python -c "import numba; print(f'numba version: {numba.__version__}')"
        python -c "import gsply.writer as w; import gsply.reader as r; print(f'HAS_NUMBA: writer={w.HAS_NUMBA}, reader={r.HAS_NUMBA}')"

    - name: Create test data
      run: |
        python -c "
        import numpy as np
        from pathlib import Path
        import gsply

        # Create directory structure expected by test_utils
        test_dir = Path('export_with_edits')
        test_dir.mkdir(exist_ok=True)

        # Generate synthetic Gaussian data
        num_gaussians = 50000
        np.random.seed(42)  # Reproducible data
        means = np.random.randn(num_gaussians, 3).astype(np.float32) * 10
        scales = np.abs(np.random.randn(num_gaussians, 3).astype(np.float32))
        quats = np.random.randn(num_gaussians, 4).astype(np.float32)
        quats = quats / np.linalg.norm(quats, axis=1, keepdims=True)
        opacities = np.random.randn(num_gaussians).astype(np.float32)
        sh0 = np.random.randn(num_gaussians, 3).astype(np.float32)
        shN = np.random.randn(num_gaussians, 15, 3).astype(np.float32)

        # Write test file in expected location
        test_file = test_dir / 'frame_00000.ply'
        gsply.plywrite(str(test_file), means, scales, quats, opacities, sh0, shN)
        print(f'Created test file: {test_file} ({test_file.stat().st_size / 1024 / 1024:.2f}MB)')

        # Also create a copy for direct file argument
        gsply.plywrite('test_data.ply', means, scales, quats, opacities, sh0, shN)
        "

    - name: Run standard benchmark
      run: |
        python benchmarks/benchmark.py --file test_data.ply --iterations 10 2>&1 | tee benchmark_results.txt
      continue-on-error: true

    - name: Run compressed benchmark
      run: |
        python benchmarks/benchmark_compressed_speedup.py 2>&1 | tee benchmark_compressed_results.txt
      continue-on-error: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark_results.txt
          benchmark_compressed_results.txt

    - name: Comment benchmark results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let results = '';
          try {
            results += '### Standard Benchmark\n\n```\n';
            results += fs.readFileSync('benchmark_results.txt', 'utf8');
            results += '\n```\n\n';
          } catch (e) {
            results += 'Standard benchmark failed\n\n';
          }
          try {
            results += '### Compressed Benchmark\n\n```\n';
            results += fs.readFileSync('benchmark_compressed_results.txt', 'utf8');
            results += '\n```\n\n';
          } catch (e) {
            results += 'Compressed benchmark failed\n\n';
          }
          const body = `## Benchmark Results\n\n${results}`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });
      continue-on-error: true
