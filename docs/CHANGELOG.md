# Release Notes

## v0.1.0 (Current - Optimized Release)

### Features
- Ultra-fast Gaussian Splatting PLY I/O library
- Pure Python + numpy (zero C++ dependencies)
- Optional numba JIT for parallel processing (graceful fallback without numba)
- Support for SH degrees 0-3 (14, 23, 38, 59 properties)
- Auto-format detection (uncompressed vs compressed)
- Full compressed format support (PlayCanvas compatible) - read AND write
- Zero-copy reads for maximum performance
- GSData namedtuple container for clean API

### Performance (Real-World Benchmarks)

Tested on 90 files, 36M Gaussians total, SH degree 0:

**Uncompressed I/O**:
- **Read** (zero-copy): 8.09ms for 400K Gaussians (49M Gaussians/sec)
- **Write**: 8.72ms vs 12.18ms (plyfile) - 1.4x faster

**Compressed I/O**:
- **Read**: 14.74ms for 400K Gaussians (27M Gaussians/sec)
- **Write**: 63ms for 400K Gaussians (6.3M Gaussians/sec)
- **Compression ratio**: 3.44x (1.92 GB â†’ 558 MB)

**Comparison vs Other Libraries** (50K Gaussians, SH3):
- Read: 2.89ms vs 18.23ms (plyfile) - **6.3x faster**
- Write: 8.72ms vs 12.18ms (plyfile) - **1.4x faster**

### Optimizations Implemented

#### Phase 1: Vectorization
- Vectorized quaternion extraction (eliminated Python loops)
- Performance: 21% improvement

#### Phase 2: Algorithmic Optimization
- O(n log n) chunk bounds computation (replaced O(n*m) boolean masking)
- Performance: 73% improvement

#### Phase 3: Radix Sort + Parallel Processing
- O(n) radix sort for chunk sorting (vs O(n log n) comparison sort)
- Parallel JIT processing with numba for bit packing/unpacking
- Performance: 10.4x write, 15.3x read vs baseline

**Combined Impact**: Production-ready performance with 60+ FPS capability

### API

```python
import gsply

# Read PLY file (auto-detects format, returns GSData namedtuple)
data = gsply.plyread("scene.ply")  # Uses fast zero-copy by default
data = gsply.plyread("scene.ply", fast=False)  # Safe copies

# Access via attributes
positions = data.means
colors = data.sh0

# Or unpack if needed
means, scales, quats, opacities, sh0, shN = data[:6]

# Write uncompressed PLY
gsply.plywrite("output.ply", data.means, data.scales, data.quats,
               data.opacities, data.sh0, data.shN)

# Write compressed PLY (auto-adjusts extension to .compressed.ply)
gsply.plywrite("output.ply", data.means, data.scales, data.quats,
               data.opacities, data.sh0, data.shN, compressed=True)

# Detect format
is_compressed, sh_degree = gsply.detect_format("scene.ply")
```

### Documentation
- Comprehensive README with performance benchmarks
- OPTIMIZATION_SUMMARY.md: Detailed optimization history and analysis
- GUIDE.md: User guide with examples
- CONTRIBUTING.md: Contribution guidelines
- Extensive inline code documentation

### Testing
- 65 passing tests
- Full coverage of read/write operations
- Compressed format tests (read + write)
- Format detection tests
- Round-trip verification
- Edge case handling

### Distribution
- Universal wheel (`py3-none-any`)
- Works on Linux, macOS, Windows
- No platform-specific compilation required
- PEP 561 type hints marker included
- Optional dependency: numba (for parallel processing)

### Known Limitations
- Requires Python 3.10+
- ASCII PLY format not supported (binary little-endian only)
- Compressed format is lossy (chunk-based quantization)
- Parallel processing requires numba (graceful fallback provided)

### Installation

```bash
# Basic installation
pip install gsply

# With optional JIT optimization
pip install gsply[jit]

# Development installation
pip install -e .[dev]

# All optional dependencies
pip install -e .[all]
```

### Dependencies
- **Required**: numpy>=1.20.0
- **Optional**: numba>=0.59.0 (for parallel processing)
- **Dev**: pytest, pytest-cov, build, twine
- **Benchmark**: open3d, plyfile

### Contributors
- OpsiClear

### License
MIT License

---

## Previous Development Phases

### Phase 1: Initial Implementation
- Basic uncompressed PLY read/write
- Format detection
- SH degree support (0-3)

### Phase 2: Compressed Format Support
- PlayCanvas compressed PLY reading
- Vectorized bit unpacking
- 38.5x speedup over naive Python loops

### Phase 3: Compressed Writing
- Compressed PLY writing implementation
- Vectorized quaternion extraction
- O(n log n) chunk bounds computation
- 78.6% speedup vs initial implementation

### Phase 4: Parallel Optimization (Current)
- O(n) radix sort for chunk sorting
- Parallel JIT processing with numba
- Graceful fallback for environments without numba
- 10.4x write, 15.3x read speedup vs baseline
- Production-ready performance (60+ FPS capable)
